{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 19:55:25.722049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-01 19:55:28.243592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-01 19:55:28.693107: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-01 19:55:28.708387: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-01 19:55:30.726229: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-01 19:58:09.037275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras_nlp.layers import TransformerDecoder\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='data/'\n",
    "plots_dir='plots/'\n",
    "output_dir='outputs/'\n",
    "val_size=0.05\n",
    "batch_size=4\n",
    "embedding_dim=32\n",
    "num_heads=4\n",
    "num_tranformer_decoders=8\n",
    "initial_learning_rate = 1e-3\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in [plots_dir, output_dir]:\n",
    "    os.makedirs(item, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, data):\n",
    "        self.vocab = self._build_vocab(data)\n",
    "        self.inverse_vocab = dict([index, token] for token, index in self.vocab.items())\n",
    "        \n",
    "    def _build_vocab(self, data):\n",
    "        vocab = {'':0, '<unk>':1}\n",
    "        for row in data:\n",
    "            for token in row.split():\n",
    "                if token in vocab:\n",
    "                    continue\n",
    "                else:\n",
    "                    vocab[token] = len(vocab)\n",
    "        return vocab\n",
    "                    \n",
    "    def encode(self, data):\n",
    "        encoded_data = []\n",
    "        for row in data:\n",
    "            encoded_row = []\n",
    "            for token in row.split():\n",
    "                encoded_row.append(self.vocab.get(token, self.vocab['<unk>']))\n",
    "            encoded_data.append(encoded_row)\n",
    "        return encoded_data\n",
    "\n",
    "    def decode(self, data):\n",
    "        decoded_data = []\n",
    "        for row in data:\n",
    "            decoded_row = []\n",
    "            for index in row:\n",
    "                decoded_row.append(self.inverse_vocab[index])\n",
    "            decoded_data.append((' '.join(decoded_row)).strip())\n",
    "        return decoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_dir, batch_size, val_size):\n",
    "    \n",
    "    with open(os.path.join(data_dir, 'Train_input'), 'rb') as file:\n",
    "        lang1 = pickle.load(file)\n",
    "    with open(os.path.join(data_dir, 'Train_output'), 'rb') as file:\n",
    "        lang2 = pickle.load(file)\n",
    "\n",
    "\n",
    "    corpus = ['<spl> ' + sen1 + '<spl> ' + sen2 + '<spl>' for sen1, sen2 in zip(lang1, lang2)]\n",
    "    train_corpus, val_corpus = train_test_split(corpus, test_size = val_size, random_state = 36)\n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(train_corpus)\n",
    "    train_encoded = tokenizer.encode(train_corpus)\n",
    "    val_encoded = tokenizer.encode(val_corpus)\n",
    "\n",
    "\n",
    "    max_length = max(len(seq) for seq in train_encoded + val_encoded)\n",
    "    train_padded = np.array(pad_sequences(train_encoded, maxlen=max_length, padding='pre'))\n",
    "    val_padded = np.array(pad_sequences(val_encoded, maxlen=max_length, padding='pre'))\n",
    "\n",
    "\n",
    "    train_in, train_label = train_padded[:, :-1], train_padded[:, 1:]\n",
    "    val_in, val_label = val_padded[:, :-1], val_padded[:, 1:]\n",
    "\n",
    "\n",
    "    train_label = keras.utils.to_categorical(train_label, num_classes=len(tokenizer.vocab))\n",
    "    val_label = keras.utils.to_categorical(val_label, num_classes=len(tokenizer.vocab))\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_in, train_label)).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_in, val_label)).batch(batch_size)\n",
    "\n",
    "    return train_dataset, val_dataset, tokenizer, max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class LLM(keras.Model):\n",
    "    def __init__(self, vocab_size, max_length, embedding_dim, num_heads, num_tranformer_decoders, **kwargs):\n",
    "        super(LLM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_tranformer_decoders = num_tranformer_decoders\n",
    "\n",
    "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
    "        self.tranformer_decoders = [TransformerDecoder(intermediate_dim=embedding_dim, num_heads=num_heads) for _ in range(num_tranformer_decoders)]\n",
    "        self.dense = Dense(vocab_size, activation='softmax') \n",
    "\n",
    "    def call(self, inputs):\n",
    "        outs = self.embedding(inputs)\n",
    "        for layer in self.tranformer_decoders:\n",
    "            outs = layer(outs)\n",
    "        outs = self.dense(outs)\n",
    "        \n",
    "        return outs\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config().copy()\n",
    "        base_config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'max_length': self.max_length,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'num_heads' : self.num_heads,\n",
    "            'num_tranformer_decoders' : self.num_tranformer_decoders\n",
    "        })\n",
    "        return base_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, save_name):\n",
    "\n",
    "    def plot_loss(history, save_name):\n",
    "        plt.plot(history.history['loss'], label='loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(save_name)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_accuracy(history, save_name):\n",
    "        plt.plot(history.history['categorical_accuracy'], label='accuracy')\n",
    "        plt.plot(history.history['val_categorical_accuracy'], label='val_accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.savefig(save_name)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_lr(history, save_name):\n",
    "        plt.plot(history.history['learning_rate'], label='Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "        plt.savefig(save_name)\n",
    "        plt.close()\n",
    "\n",
    "    plot_loss(history, save_name + 'loss.png')\n",
    "    plot_accuracy(history, save_name + 'accuracy.png')\n",
    "    plot_lr(history, save_name + 'lr.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 20:05:26.374650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21300 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722560747.381137  253770 service.cc:146] XLA service 0x7fb348002340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1722560747.381281  253770 service.cc:154]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\n",
      "2024-08-01 20:05:47.821842: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-01 20:05:52.241209: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-08-01 20:06:06.435513: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_19', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "I0000 00:00:1722560766.551598  253770 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, tokenizer, max_length = create_dataset(data_dir, batch_size, val_size)\n",
    "\n",
    "model = LLM(len(tokenizer.vocab), max_length, embedding_dim, num_heads, num_tranformer_decoders)\n",
    "model.compile(optimizer=Adam(learning_rate=initial_learning_rate),\n",
    "              loss=CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[CategoricalAccuracy()])\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch!=0 and epoch%3==0:\n",
    "        return lr*0.7\n",
    "    return lr\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data = val_dataset,\n",
    "                    epochs = epochs,\n",
    "                    callbacks=[keras.callbacks.LearningRateScheduler(scheduler)],\n",
    "                    batch_size=batch_size,\n",
    "                    verbose = 50)\n",
    "\n",
    "\n",
    "plot(history, plots_dir)\n",
    "\n",
    "with open(output_dir + 'tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(output_dir + 'model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Testing Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating Sequences...: 100%|██████████| 2/2 [00:16<00:00,  8.44s/it]\n"
     ]
    }
   ],
   "source": [
    "with open('data/Train_input', 'rb') as file:\n",
    "    lang1 = pickle.load(file)\n",
    "\n",
    "with open('data/Train_output', 'rb') as file:\n",
    "    lang2 = pickle.load(file)\n",
    "\n",
    "with open(output_dir + 'tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "with open(output_dir + 'model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "\n",
    "data_input = ['<spl> ' + sen + '<spl> ' for sen in lang1]\n",
    "data_label = lang2\n",
    "\n",
    "_, data_test = train_test_split(list(zip(data_input, data_label)), test_size = val_size, random_state = 36)\n",
    "data_test = data_test[:2]\n",
    "data_test = list(zip(*data_test))\n",
    "test_input, test_label = data_test\n",
    "test_input = tokenizer.encode(test_input)\n",
    "test_input = pad_sequences(test_input, maxlen=max_length, padding='pre')\n",
    "\n",
    "encoded_out = []\n",
    "for item in tqdm(test_input, desc='Translating Sequences...'):\n",
    "    item = np.expand_dims(item, axis=0)\n",
    "    pred_toks = []\n",
    "    for _ in range(60):\n",
    "        pred_probs = model.predict(item, verbose=0)  # Returns an array of shape (1, 81, vocab_size)\n",
    "        pred_tok = np.argmax(pred_probs[0], axis = -1)[-1]\n",
    "\n",
    "        if pred_tok == tokenizer.vocab[\"<spl>\"]:\n",
    "            break\n",
    "\n",
    "        item = np.append(item[:, 1:], np.expand_dims([pred_tok], axis=0), axis=-1)\n",
    "        pred_toks.append(pred_tok)\n",
    "\n",
    "    encoded_out.append(pred_toks)\n",
    "\n",
    "\n",
    "encoded_out = [[tok for tok in seq if tok not in [tokenizer.vocab[\"<unk>\"], tokenizer.vocab[\"\"]]] for seq in encoded_out ]\n",
    "\n",
    "out = tokenizer.decode(encoded_out)\n",
    "\n",
    "correct = 0\n",
    "for predicted, label in zip(out, test_label):\n",
    "    if predicted == label:\n",
    "        correct+=1\n",
    "\n",
    "#print('Test Generation Accuracy = ', correct/len(test_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3505529a510d0f256a2a0e71ad411f12af29ab3a4c178ad962716245f26b1c59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
